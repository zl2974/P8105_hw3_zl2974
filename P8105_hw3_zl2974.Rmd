---
title: "P8105_hw3_zl2974"
author : "Jeffrey Liang"
date : "10/02/2020"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rnoaa)
library(ggridges)
library(patchwork) # use to multi plot, plt1 + (pl2 + plt3) / plt4
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message =F,
  warning = F
  )

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d 
```


# Problem 1

```{r p1_exploring,fig.height=8,fig.width=8}
data("instacart")
instacart %>% 
  count(aisle,name = "n_count") %>% 
  arrange(desc(n_count))

instacart %>% 
  group_by(aisle) %>% 
  filter(n()>1e+4) %>%
  mutate(counts = n()) %>% 
  ungroup() %>% 
  mutate(aisle = 
           forcats::fct_reorder(aisle,counts)) %>% 
  ggplot(aes(y = aisle,fill = aisle)) +
  geom_bar()+
  scale_x_continuous(trans = "sqrt")+
  theme(legend.position = "none")

for (aisle_ in c("baking ingredients","dog food care","packaged vegetables fruits")){
instacart %>% 
  filter(aisle %in% c(aisle_)) %>%
  count(aisle,product_name) %>% 
  slice_max(n,n=3) %>% #or use mutate(rank = min_rank(desc(n)))
  left_join(instacart) %>% 
    janitor::tabyl(aisle,product_name) %>% 
    print()
}

instacart %>% 
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(product_name,order_dow) %>% 
  summarise(avg_order_hour_of_day = mean(order_hour_of_day,na.rm = T)) %>% 
  pivot_wider(names_from = order_dow,
              names_prefix = "avg_order_hour_of_day_week_",
              values_from = avg_order_hour_of_day) %>% 
  ungroup() %>% 
  t()
```




# Problem 2
## Load data

```{r}
accelerometer =
  read_csv(here::here("data/accel_data.csv")) %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = starts_with("activity_"),
    names_to = 'min',
    values_to = "activity",
    names_prefix = "activity_"
  ) %>% 
  mutate_at(c("min"),as.numeric) %>% 
  mutate(weekday_vs_weekend = day %in% c("Sunday","Saturday"),
         weekday_vs_weekend=
           case_when(weekday_vs_weekend ~"weekend",
                     !weekday_vs_weekend ~ "weekday") %>% 
           as.factor(),
         week = as.character(week) %>%
           forcats::fct_relevel(as.character(1:5)),
         day = forcats::fct_relevel(day,
                                    c("Monday","Tuesday","Wednesday","Thursday",
                                     "Friday", "Saturday","Sunday"))
         ) %>% 
  group_by(week) %>% 
  arrange(day,.by_group=T) %>% 
  group_by(day_id) %>% 
  mutate(min_week = 1,
         hour_day = 
           cumsum(min_week)%/%30/2) %>% 
  ungroup(day_id) %>% 
  mutate(min_week=
           cumsum(min_week),
         hour_week =
           min_week%/%30/2) %>% 
  ungroup() %>% 
  select(-day_id) %>% 
  left_join(
    distinct(.,week,day,) %>% 
      ungroup() %>% 
      mutate(day_id = 1,
             day_id = cumsum(day_id))
  )

skimr::skim_without_charts(accelerometer)
```
\ The original data is a "wider" format data with `r paste(dim(read_csv(here::here("data/accel_data.csv"))),collapse = " x ")` dimension. In order to make the data compatible with machine, pivot_long() is used to make the _activity_*_ of all subjects into columns of _min and activity_. Following instruction, weekend vs weekday's variable is built on _day_ and producing factor columns with levels of `r levels(pull(accelerometer, weekday_vs_weekend))`. With careful examine, the _day_id_'s order is not correct, a new column of _day_id_ follow chronological order generated with _week_ and reordered factors _day_. Also for convenient, _hour_ of the day and _hour_week_ of the week were produced with cumsum() function. All changes resulted the data into a `r paste(dim(accelerometer),collapse = " x ")` dataset.

* Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r total_activity, warning=F, message=F}
accelerometer %>% 
  group_by(week,day) %>% 
  summarise(daily_activity = sum(activity,na.rm=T)) %>% 
  pivot_wider(names_from = week,
              names_prefix = "week ",
              values_from = daily_activity) %>% 
  knitr::kable()
```

\ A 7 by 5 table is produced. Noticing that the subject had 2 exceptional low activity on Saturday on week 4 and 5 inmediately. Activities in Wendesay and Thurday are more stable as shown in this table.

* Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r smooth_activity}
accelerometer %>% 
  group_by(day_id,day,hour_day) %>% 
  summarise(activity_hr = 
              sum(activity,na.rm=T)) %>% 
  ggplot(aes(x=hour_day,
             y= activity_hr,
             color = day,
             group = day_id))+
  stat_smooth(se = F,
              method = "loess",
              geom="line",
              alpha =0.5)+
  scale_y_continuous(trans = "sqrt",
                     name = "Hourly Activities",
                     breaks = c(2000,10000,20000))+
  scale_x_continuous(name = "Time",
                     limits = c(0,24),
                     breaks = seq(0,24,6))+
  viridis::scale_color_viridis(discrete = T,
                               name = "")+
  guides(color=guide_legend(nrow=1,
                            byrow=TRUE))+
  labs(
    title = "Smooth Line for Hourly activities in 35 days",
    caption = "fig.2.1"
  )
```

\ A smooth line plot of 24hr activities is made, most of the lines are overlapping indicate some general pattern of the subject's daily routine: activities peak after 8 am and start decreasing after 6 pm. Few outliers were observed as well, some Saturday's activities are unfluctuated, and some sunday's activities and monday's as well as Friday's peak later than the most of other days'.


```{r activity_plot,fig.height=6,fig.width=8,message=F}
accelerometer %>% 
  group_by(week,day_id,hour_day) %>% 
  summarise(activity_hr = 
              sum(activity,na.rm=T)) %>% 
  ggplot(aes(x=hour_day,
             y=day_id,
             fill=activity_hr))+
  geom_raster(stat="identity")+
  scale_y_continuous(breaks = seq(3,31,7),
                   labels = c(str_c("week",c(1:5))),
                   position = "left",
                   name = '')+
  scale_x_continuous(breaks = seq(0,24,6),
                     name = "Hour of day")+
  theme(legend.position = "right")+
  viridis::scale_fill_viridis(option = "C",
                              name = "Hourly\nActivities",
                              trans = "sqrt")+
  labs(
    title = "Hourly activities in 35 days",
    caption = "fig.2.2"
  )

```

\ To display more information, a heatmap is made. In Week 1, the first few days show less changes in activities between hour compared to the following weeks, especially the first day of week 1, the researcher can look into if the subject have the equipment gear up properly. In the following weeks, subject has his/her first peek of activities around 6-10, and following the second peek around 20. The activities on Saturday on week 4 and 5 show no changes in the heatmap, coherent with the finding in 7 by 5 table. The sunday in weeks 4 and 5 and the Firday in week 4 show less activities, accordinates with the outliers displayed in Fig.2.2.

# Problem 3
```{r load_noaa}
data("ny_noaa")
ny_noaa_tidy = ny_noaa %>% 
    separate(date,
             into=c("year","month","day"),sep="-",
             remove = F) %>%
    mutate(across(year:tmin,as.numeric),
           across(tmax:tmin,function(x) x/10)) %>% 
    mutate(
      snow = case_when(
      snow <0 ~0,
      snow >= 0 ~snow))
    
skimr::skim_without_charts(ny_noaa_tidy)
```
\ The data of NYC's noaa is a tidy `r paste(dim(ny_noaa_tidy),collapse = "x ")` data in term's of structure, collecting data from `r min(pull(ny_noaa_tidy, date))` to 
`r max(pull(ny_noaa_tidy, date))` of `r n_distinct(pull(ny_noaa_tidy,id))` stations. But missing value in _tmax_ and _tmin_ have exceed 40% and around 20% in _prcp_ and _snow_ columns, in other words, simplily omitting NA will drop at least 40% of the data.

Impossible outlier is observed in _snow_, which most common observation is 0 after omitting Na value, which can be interpret as no snow in that day. Across 3 decades, the mean of highest temperature is 14.0 celcious and mean lowest of 3.03 celcious. 

* Make a two-panel plot showing the average max temperature in January and in July in *each station* across years. Is there any observable / interpretable structure? Any outliers?

```{r plot_2_panel_max, fig.height=8}
plt_1 = 
ny_noaa_tidy %>% 
  filter(as.numeric(month) %in% c(1)) %>% 
  group_by(month,year,id) %>% 
  summarise(tmax_avg = mean(tmax,na.rm = T)) %>% 
  drop_na() %>% 
  ggplot(aes(x = year,
             y = tmax_avg,
             color = id,
             group = id))+
  geom_point(alpha = 0.3)+
  geom_path(alpha = 0.3)+
  theme(legend.position = 'none',
        axis.title.x = element_blank())+
  scale_x_continuous(breaks = seq(1981,2010,2),
                     limits = c(1981,2010))+
  theme(axis.text.x = element_text(angle = -90,
                                   vjust = 0.5,
                                   hjust = 1))+
  labs(title = "Average Highest Temperature in January and July from 1981-2010",
       y = "Average Highest Temperature in January")

plt_2 =
  ny_noaa_tidy %>% 
  filter(as.numeric(month) %in% c(7)) %>% 
  group_by(month,year,id) %>% 
  summarise(tmax_avg = mean(tmax,na.rm = T)) %>% 
  drop_na() %>% 
  ggplot(aes(x = year,
             y = tmax_avg,
             color = id,
             group = id))+
  geom_point(alpha = 0.3)+
  geom_path(alpha = 0.3)+
  theme(legend.position = 'none')+
  labs(y = "Average Highest Temperature in July")+
  scale_x_continuous(breaks = seq(1981,2010,2),
                     limits = c(1981,2010))+
  theme(axis.text.x = element_text(angle = -90,
                                   vjust = 0.5,
                                   hjust = 1))

plt_1 / plt_2

outlier = 
  ny_noaa_tidy %>% 
    group_by(month,year,id) %>% 
    summarise(tmax_avg = mean(tmax,na.rm = T)) %>% 
    ungroup(id) %>% 
    mutate(upper = tmax_avg >quantile(tmax_avg,.75,na.rm = TRUE)+1.5*IQR(tmax_avg,na.rm = TRUE),
           lower = tmax_avg <quantile(tmax_avg,.25,na.rm = TRUE)-1.5*IQR(tmax_avg,na.rm = TRUE)) %>%
    ungroup() %>% 
    summarise(upper = sum(upper,na.rm = T),
              lower = sum(lower,na.rm = T))
```

\ January's average Highest temperatures observed in NYC across all stations differs around 10 degrees across the time. Two exceptional cold years in terms of highest temperature are observed in all station in 
`r ny_noaa_tidy %>% filter(month == 1)%>% group_by(year) %>% summarise(tmax = (mean(tmax,na.rm = T))) %>% mutate(rank = min_rank(tmax)) %>% slice_min(rank,n=2) %>% pull(year) %>% paste(collapse=" and ")`
. \ Mean Highest temperatures in July flucturate arond 20 to 30, the measure difference across stations is similar to January. Outliers were observed in January in 3 decades, as well as in July, if define outliers as observations lie 1.5 times IQR away from 1st and 3rd quarter, we have `r paste(outlier,collapse = ", ")` upper and lower outlier.


```{r t_n_snow, fig.width=8,fig.height=8}
plt_1 = 
ny_noaa_tidy %>%
    pivot_longer(
      tmax:tmin,
      names_to = "observation",
      values_to = "temperature"
    ) %>% 
    mutate(observation = forcats::fct_relevel(observation,c("tmin","tmax"))) %>% 
    ggplot(aes(x = as.factor(year), y = temperature, color = observation))+
    geom_boxplot(outlier.size = 0.2)+
    scale_x_discrete(breaks = seq(1981,2010,1),
                     name = "Year")+
    viridis::scale_color_viridis(discrete = T,
                                 option = "D",
                                 name = "Observation")+
    labs(y = "Temperature(C)",
         caption = "Fig 3.2")+
    theme(axis.text.x = element_text(angle = -90,
                                     vjust = 0.5,
                                     hjust = 1))

plt_2 = 
ny_noaa_tidy %>% 
  filter(between(snow,1,100),
         !is.na(snow)) %>% 
  ggplot(aes(x= snow))+
  geom_density_ridges(aes(y = year,
                          group = as.factor(year)),
                          alpha = 0.2,
                      rel_min_height = 0.15)+
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = -90,
                                   vjust = 0.5,
                                   size = 8,
                                   hjust = 1))+
  scale_x_continuous(trans = "log",
                     breaks = c(5, 15,40, 100))+
  scale_y_continuous(breaks = seq(1981,2010,1),
                     limits = c(1981,NA))+
  coord_flip()+
  labs(title = "Density for snowfall and Boxplot for Temperature from 1908 to 2010",
       x = "Snowfall(mm)")

(plt_2 / plt_1)+ plot_layout(guides = 'collect',widths = 8, heights = 16)
```

\ Grouped Boxplot is used to show pattern and extreme tempertures in the whole dataset. Although mean and IQR of the lowest and highest temperatures haven't showed any sign of global warming, extreme lowest temperature is occurring less often in 2000s compared to before 2000. Density ridges with flipped coordinate is use to show snowfall pattern. The kernel density becomes flatten in 2009 and 2010. Also, light snowfall(<=10 mm) is taking higher proportion of total snowfall. So, some evidence favor the theorem of global warming might be within this data.
