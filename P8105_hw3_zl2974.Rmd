---
title: "P8105_hw3_zl2974"
author : "Jeffrey Liang"
date : "10/02/2020"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rnoaa)
library(ggridges)
library(patchwork) # use to multi plot, plt1 + (pl2 + plt3) / plt4
library(tidyverse)

knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message =F,
  warning = F
  )

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d 
```


# Problem 1
```{r}
library(p8105.datasets)
data("instacart")
```
```{r p1_exploring,fig.height=8,fig.width=8}
instacart %>% 
  count(aisle,name = "n_count") %>% 
  arrange(desc(n_count))

instacart %>% 
  group_by(aisle) %>% 
  filter(n()>1e+4) %>%
  mutate(counts = n()) %>% 
  ungroup() %>% 
  mutate(aisle = 
           forcats::fct_reorder(aisle,counts)) %>% 
  ggplot(aes(y = aisle,fill = aisle)) +
  geom_bar()+
  scale_x_continuous(trans = "sqrt")+
  theme(legend.position = "none")

for (aisle_ in c("baking ingredients","dog food care","packaged vegetables fruits")){
instacart %>% 
  filter(aisle %in% c(aisle_)) %>%
  count(aisle,product_name) %>% 
  slice_max(n,n=3) %>% #or use mutate(rank = min_rank(desc(n)))
  left_join(instacart) %>% 
    janitor::tabyl(aisle,product_name) %>% 
    print()
}

instacart %>% 
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(product_name,order_dow) %>% 
  summarise(avg_order_hour_of_day = mean(order_hour_of_day,na.rm = T)) %>% 
  pivot_wider(names_from = order_dow,
              names_prefix = "avg_order_hour_of_day_week_",
              values_from = avg_order_hour_of_day) %>% 
  ungroup() %>% 
  t()
```




# Problem 2
## Load data

```{r}
accelerometer =
  read_csv(here::here("data/accel_data.csv")) %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = starts_with("activity_"),
    names_to = 'min',
    values_to = "activity",
    names_prefix = "activity_"
  ) %>% 
  mutate_at(c("min"),as.numeric) %>% 
  mutate(weekday_vs_weekend = day %in% c("Sunday","Saturday"),
         weekday_vs_weekend=
           case_when(weekday_vs_weekend ~"weekend",
                     !weekday_vs_weekend ~ "weekday") %>% 
           as.factor(),
         week = as.character(week) %>% forcats::fct_relevel(as.character(1:5)),
         day = forcats::fct_relevel(day,c("Monday","Tuesday","Wednesday","Thursday",
                                     "Friday", "Saturday","Sunday"))
         ) %>% 
  group_by(week) %>% 
  arrange(day,.by_group=T) %>% 
  group_by(day_id) %>% 
  mutate(min_week = 1,
         hour_day = cumsum(min_week)%/%30/2) %>% 
  ungroup(day_id) %>% 
  mutate(min_week=cumsum(min_week),
         hour_week = min_week%/%30/2) %>% 
  ungroup() %>% 
  select(-day_id) %>% 
  left_join(
    distinct(.,week,day,) %>% 
      ungroup() %>% 
      mutate(day_id = 1,day_id = cumsum(day_id))
  )
skimr::skim_without_charts(accelerometer)
```
\ The original data is a "wider" format data with `r paste(dim(read_csv(here::here("data/accel_data.csv"))),collapse = " x ")` dimension. In order to make the data compatible with machine, pivot_long() is used to make the _activity_*_ of all subjects into columns of _min and activity_. Following instruction, weekend vs weekday's variable is build on _day_id_ and result into a `r paste(dim(accelerometer),collapse = " x ")` dataset.

* Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r total_activity, warning=F, message=F}
accelerometer %>% 
  group_by(week,day) %>% 
  summarise(daily_activity = sum(activity,na.rm=T)) %>% 
  pivot_wider(names_from = week,
              names_prefix = "week ",
              values_from = daily_activity) %>% 
  knitr::kable()
# TODO: hwo to make this table clean and nice? definatly not janitor, this is continuous
```

* Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r}
accelerometer %>% 
  group_by(day_id,day,hour_day) %>% 
  summarise(activity_hr = 
              sum(activity,na.rm=T)) %>% 
  ggplot(aes(x=hour_day,
             y= activity_hr,
             color = day,
             group = day_id))+
  geom_smooth(se = F)+
  scale_y_continuous(trans = "sqrt",
                     name = "Hourly Activities")+
  scale_x_continuous(name = "Time")+
  viridis::scale_color_viridis(discrete = T,
                               name = "")+
  guides(color=guide_legend(nrow=1,
                            byrow=TRUE))+
  labs(
    title = "Hourly activities in 35 days"
  )
```


```{r activity_plot,fig.height=6,fig.width=8,message=F}
accelerometer %>% 
  group_by(week,day_id,hour_day) %>% 
  summarise(activity_hr = 
              sum(activity,na.rm=T)) %>% 
  ggplot(aes(x=hour_day,
             y=day_id,
             fill=activity_hr))+
  geom_raster(stat="identity")+
  scale_y_continuous(breaks = seq(3,31,7),
                   labels = c(str_c("week",c(1:5))),
                   position = "left",
                   name = '')+
  scale_x_continuous(breaks = seq(0,24,6),
                     name = "Hour of day")+
  theme(legend.position = "right")+
  viridis::scale_fill_viridis(option = "C",
                              name = "Hourly\nActivities",
                              trans = "sqrt")+
  labs(
    title = "Hourly activities in 35 days"
  )

```


# Problem 3
```{r load_noaa}
read_ny_noaa = function(){
  ny_noaa_tidy = "Not  Tibble"
ny_noaa_tidy = read_csv(here::here("data/ny_noaa_tidy.csv"),
                        col_types = "cDdddddddd")
if(!is_tibble(ny_noaa_tidy)){
    print("Can't load data")
    data("ny_noaa")
    ny_noaa_tidy = ny_noaa %>% 
    separate(date,
             into=c("year","month","day"),sep="-",
             remove = F) %>%
    mutate(across(year:tmin,as.numeric),
           across(tmax:tmin,function(x) x/10)) %>% 
    mutate(
      snow = case_when(
      snow <0 ~0,
      snow >= 0 ~snow))
  write_csv(ny_noaa_tidy,here::here("data/ny_noaa_tidy.csv"))
    }
return(ny_noaa_tidy)
}
ny_noaa_tidy = read_ny_noaa()
skimr::skim_without_charts(ny_noaa_tidy %>% select(-id))

```

\ Most common observation of _snow_ is 0 omitting Na value, which representing no snow in that day.

* Make a two-panel plot showing the average max temperature in January and in July in *each station* across years. Is there any observable / interpretable structure? Any outliers?

```{r plot_2_panel_max, fig.height=8}
plt_1 = 
ny_noaa_tidy %>% 
  filter(as.numeric(month) %in% c(1)) %>% 
  group_by(month,year,id) %>% 
  summarise(tmax_avg = mean(tmax,na.rm = T)) %>% 
  mutate(rank  = min_rank(desc(tmax_avg))
         ) %>%
  drop_na() %>% 
  ggplot(aes(x = year,
             y = tmax_avg,
             color = id,
             group = id))+
  geom_point(alpha = 0.3)+
  geom_path(alpha = 0.3)+
  theme(legend.position = 'none',
        axis.title.x = element_blank())+
  scale_x_discrete(labels = c())+
  labs(title = "Average Highest Temperature in January and July from 1981-2010",
       y = "Average Highest Temperature in Janurary")

plt_2 =
  ny_noaa_tidy %>% 
  filter(as.numeric(month) %in% c(7)) %>% 
  group_by(month,year,id) %>% 
  summarise(tmax_avg = mean(tmax,na.rm = T)) %>% 
  mutate(rank  = min_rank(desc(tmax_avg))
         ) %>%
  drop_na() %>% 
  ggplot(aes(x = year,
             y = tmax_avg,
             color = id,
             group = id))+
  geom_point(alpha = 0.3)+
  geom_path(alpha = 0.3)+
  theme(legend.position = 'none')+
  labs(y = "Average Highest Temperature in July")

plt_1 / plt_2
```


```{r tmax_vs_tmin, eval = F}
ny_noaa_tidy %>% 
  filter(as.numeric(month) %in% c(1,7)) %>% 
  mutate(day = as.factor(day),
         month = month.name[month]) %>% 
  ggplot(aes(x = day, y = tmax, color = month)) +
  geom_boxplot(outlier.size = 0.5,
               outlier.alpha = 0.5)+
  scale_color_viridis_d(option = "E",name = "Month")+
  facet_wrap(month~.,nrow = 1)+
  scale_y_continuous(position = "right",
                     name = "Highest Temperature of the Day")+
  scale_x_discrete(breaks = seq(1,31,5),
                   name = "Day of Month")

plt_1=
  ny_noaa_tidy %>%
    mutate(date = lubridate::floor_date(date, unit = "month")) %>% 
    filter(year < 1996) %>% 
    group_by(year,month,date) %>% 
    summarise(tmax = mean(tmax,na.rm = T),
              tmin = mean(tmin,na.rm = T)) %>% 
    ungroup() %>% 
    ggplot(aes(x = date,
               color = tmin - median(tmin)))+
    geom_linerange(aes(ymax = tmax,
                   ymin = tmin),
               na.rm=T)+
    theme(legend.position = 'none')+
    scale_x_date(name = '')
    #geom_point(aes(y = tmax),shape = 6)+
    #geom_point(aes(y = tmin),shape = 2)

plt_2=
  ny_noaa_tidy %>%
    mutate(date = lubridate::floor_date(date, unit = "month")) %>% 
    filter(year >= 1996) %>% 
    group_by(year,month,date) %>% 
    summarise(tmax = mean(tmax,na.rm = T),
              tmin = mean(tmin,na.rm = T)) %>% 
    ungroup() %>% 
    ggplot(aes(x = date,
               color = tmin - median(tmin)))+
    geom_linerange(aes(ymax = tmax,
                   ymin = tmin),
               na.rm=T)+
    scale_x_date(name = '')
    #geom_point(aes(y = tmax),shape = 6)+
    #geom_point(aes(y = tmin),shape = 2)

show(plt_1/plt_2)
```


```{r, echo = F, eval = F}
  ny_noaa_tidy %>%
    mutate(date = lubridate::floor_date(date, unit = "month")) %>% 
    group_by(year,month,date) %>% 
    summarise(tmax = mean(tmax,na.rm = T),
              tmin = mean(tmin,na.rm = T)) %>% 
    ungroup() %>% 
    ggplot(aes(x = date,
               color = tmin - median(tmin)))+
    geom_linerange(aes(ymax = tmax,
                   ymin = tmin),
               na.rm=T)+
    scale_x_date(name = '')
    #geom_point(aes(y = tmax),shape = 6)+
    #geom_point(aes(y = tmin),shape = 2)

plt_1 = 
ny_noaa_tidy %>%
  group_by(year) %>% 
    summarise(tmax = median(tmax,na.rm = T),
              tmin = median(tmin,na.rm = T)) %>% 
    ungroup() %>% 
    pivot_longer(
      tmax:tmin,
      names_to = "observation",
      values_to = "temperature"
    ) %>% 
    ggplot(aes(x = year, y = temperature, color = observation))+
    geom_line()
```


```{r t_n_snow, fig.width=8,fig.height=8}
plt_1 = 
ny_noaa_tidy %>%
    pivot_longer(
      tmax:tmin,
      names_to = "observation",
      values_to = "temperature"
    ) %>% 
    mutate(observation = forcats::fct_relevel(observation,c("tmin","tmax"))) %>% 
    ggplot(aes(x = as.factor(year), y = temperature, color = observation))+
    geom_boxplot(outlier.size = 0.2)+
    scale_x_discrete(breaks = c(1980,1990,2000,2010),
                     name = "Year")+
    viridis::scale_color_viridis(discrete = T,
                                 option = "D",
                                 name = "Observation")+
    labs(y = "Temperature(C)")

plt_2 = 
ny_noaa_tidy %>% 
  filter(between(snow,1,100),
         !is.na(snow)) %>% 
  ggplot(aes(x= snow))+
  geom_density_ridges(aes(y = year,
                          group = as.factor(year)),
                          alpha = 0.2,
                      rel_min_height = 0.15)+
  theme(axis.title.x = element_blank())+
  scale_x_continuous(trans = "log",
                     breaks = seq(0,100,30),
                     position = "top")+
  coord_flip()+
  labs(title = "Density for snowfall and Boxplot for Temperature from 1908 to 2010",
       x = "Snowfall(mm)")
(plt_2 / plt_1)+ plot_layout(guides = 'collect',widths = 8, heights = 16)
```

